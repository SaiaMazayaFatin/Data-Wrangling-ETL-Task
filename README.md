# ETL Pipeline - Log Data Cleaning

A complete ETL (Extract, Transform, Load) pipeline for cleaning and processing dirty log data using **Prefect** for workflow orchestration.

---

## ğŸ“ Project Structure

```
ETL/
â”œâ”€â”€ main_flow.py              # Main Prefect flow orchestrator
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ dirty_logs.txt    # Raw input data (dirty logs)
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ cleaned_data.csv  # Cleaned output data
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_eda_initial_check.ipynb  # Exploratory Data Analysis
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py            # Data extraction task
â”‚   â”œâ”€â”€ transform.py          # Data transformation & validation
â”‚   â”œâ”€â”€ load.py               # Data loading task
â”‚   â””â”€â”€ utils.py              # Utility functions
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_transforms.py    # Unit tests
```

---

## ğŸ”„ ETL Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ETL PIPELINE FLOW                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   EXTRACT    â”‚ â”€â”€â”€â–¶ â”‚  TRANSFORM   â”‚ â”€â”€â”€â–¶ â”‚     LOAD     â”‚
  â”‚              â”‚      â”‚              â”‚      â”‚              â”‚
  â”‚ dirty_logs   â”‚      â”‚  Clean &     â”‚      â”‚ cleaned_data â”‚
  â”‚   .txt       â”‚      â”‚  Validate    â”‚      â”‚    .csv      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Phase 1: Exploratory Data Analysis (EDA)

**File:** [notebooks/01_eda_initial_check.ipynb](notebooks/01_eda_initial_check.ipynb)

The EDA notebook explores the raw data to identify data quality issues:

### Findings from EDA:

| Issue | Description | Example |
|-------|-------------|---------|
| **Timestamp Formats** | Multiple formats exist | Unix (`1759380000`), ISO (`2025-10-10T13:40:12+00:00`), European (`DD/MM/YYYY`) |
| **Invalid User IDs** | Contains invalid entries | `guest`, empty strings, `null`, integers without prefix |
| **Platform Inconsistency** | Same platform, different names | `android`, `Android`, `google` (all should be `Android`) |
| **Negative Duration** | Invalid session durations | `-50`, `120s` (with suffix) |
| **Irrelevant Events** | System events to filter | `system_heartbeat`, `ad_load` |

---

## âš™ï¸ Phase 2: Extract

**File:** [src/extract.py](src/extract.py)

Reads the raw log file and converts it to a pandas DataFrame.

```python
@task(name="Extract Data")
def extract_data(file_path: str) -> pd.DataFrame
```

**Note:** The input file uses Python-style syntax (`logs = [...]`) with JSON-style `null` values, requiring special parsing logic.

---

## ğŸ”§ Phase 3: Transform

**File:** [src/transform.py](src/transform.py)

Applies data cleaning and transformation rules:

| Step | Transformation | Rule |
|------|---------------|------|
| 1 | Filter Events | Remove `system_heartbeat`, `ad_load`, `None` |
| 2 | Validate User ID | Remove `null`, empty string, `guest` |
| 3 | Standardize User ID | Convert integers to `U-XXXXX` format |
| 4 | Standardize Platform | Map to `Android`, `iOS`, `Web`, `Other` |
| 5 | Clean Duration | Remove `s` suffix, handle negatives (clip to 0) |
| 6 | Normalize Timestamp | Convert all formats to ISO 8601 UTC |
| 7 | Feature Engineering | Add `device_type` column (`Mobile`/`Desktop`/`Other`) |

### Platform Mapping:
```
android, Android, google  â†’  Android
ios, iOS, Apple           â†’  iOS
web, WebApp               â†’  Web
Others                    â†’  Other
```

### Data Integrity Checks:
- âœ… No null User IDs
- âœ… No null Timestamps
- âœ… No negative session durations

---

## ğŸ’¾ Phase 4: Load

**File:** [src/load.py](src/load.py)

Saves the cleaned DataFrame to CSV with ordered columns:

```python
@task(name="Load Data")
def load_data(df: pd.DataFrame, output_path: str)
```

**Output Columns:**
```
log_id | timestamp | user_id | device_type | device_platform | event_type | session_duration_sec
```

---

## ğŸ› ï¸ Utility Functions

**File:** [src/utils.py](src/utils.py)

| Function | Description |
|----------|-------------|
| `parse_timestamp(ts)` | Converts multiple timestamp formats to ISO 8601 UTC |
| `standardize_uid(uid)` | Converts integer User IDs to `U-XXXXX` format |

### Supported Timestamp Formats:
1. **Unix Timestamp:** `1759380000` â†’ `2025-10-01T12:00:00Z`
2. **European Format:** `01/10/2025 12:00:00` â†’ `2025-10-01T12:00:00Z`
3. **ISO 8601:** `2025-10-01T12:00:00+00:00` â†’ `2025-10-01T12:00:00Z`

---

## ğŸš€ How to Run

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run the ETL Pipeline
```bash
python main_flow.py
```

### 3. Expected Output
```
Reading data from data/raw/dirty_logs.txt...
ğŸ”„ Running transformation logic...
âœ… Data Cleaned & Validated. Rows remaining: XXXX
Saving to data/processed/cleaned_data.csv...
```

---

## ğŸ“¦ Dependencies

```
pandas
numpy
prefect
pytest
python-dateutil
```

---

## ğŸ§ª Testing

Run unit tests with pytest:
```bash
pytest tests/
```

---

## ğŸ“ˆ Data Flow Summary

```
Input: data/raw/dirty_logs.txt
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            EDA (Notebook)              â”‚
â”‚  - Identify data quality issues        â”‚
â”‚  - Analyze timestamp formats           â”‚
â”‚  - Find invalid user IDs               â”‚
â”‚  - Check platform inconsistencies      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         EXTRACT (extract.py)           â”‚
â”‚  - Read dirty_logs.txt                 â”‚
â”‚  - Parse Python-style data format      â”‚
â”‚  - Convert to DataFrame                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        TRANSFORM (transform.py)        â”‚
â”‚  - Filter irrelevant events            â”‚
â”‚  - Validate & standardize user IDs     â”‚
â”‚  - Normalize platforms                 â”‚
â”‚  - Clean session durations             â”‚
â”‚  - Parse & normalize timestamps        â”‚
â”‚  - Add device_type feature             â”‚
â”‚  - Data integrity checks               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LOAD (load.py)               â”‚
â”‚  - Reorder columns                     â”‚
â”‚  - Save to CSV                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
Output: data/processed/cleaned_data.csv
```

---

## ğŸ“ License

This project is for technical assessment purposes.
